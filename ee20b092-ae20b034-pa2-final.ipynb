{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install pyvirtualdisplay\nimport numpy as np\nimport random\nfrom collections import namedtuple, deque\nimport gym","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T06:32:32.311891Z","iopub.execute_input":"2024-04-07T06:32:32.312160Z","iopub.status.idle":"2024-04-07T06:32:45.838447Z","shell.execute_reply.started":"2024-04-07T06:32:32.312136Z","shell.execute_reply":"2024-04-07T06:32:45.837625Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\nDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nInstalling collected packages: pyvirtualdisplay\nSuccessfully installed pyvirtualdisplay-3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.distributions import Categorical\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:45.840193Z","iopub.execute_input":"2024-04-07T06:32:45.840487Z","iopub.status.idle":"2024-04-07T06:32:49.319268Z","shell.execute_reply.started":"2024-04-07T06:32:45.840461Z","shell.execute_reply":"2024-04-07T06:32:49.318299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom IPython import display as ipythondisplay","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:49.320560Z","iopub.execute_input":"2024-04-07T06:32:49.320969Z","iopub.status.idle":"2024-04-07T06:32:49.479770Z","shell.execute_reply.started":"2024-04-07T06:32:49.320943Z","shell.execute_reply":"2024-04-07T06:32:49.478998Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:49.481444Z","iopub.execute_input":"2024-04-07T06:32:49.481745Z","iopub.status.idle":"2024-04-07T06:32:50.257078Z","shell.execute_reply.started":"2024-04-07T06:32:49.481720Z","shell.execute_reply":"2024-04-07T06:32:50.256066Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Duelling DQN","metadata":{}},{"cell_type":"code","source":"GAMMA = 0.99            # discount factor\nclass DuellingDQN(nn.Module):\n\n    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64, update_type=1):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fc1_units (int): Number of nodes in first hidden layer\n            fc2_units (int): Number of nodes in second hidden layer\n        \"\"\"\n        self.update_type = update_type\n\n        super(DuellingDQN, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, fc1_units)\n        self.fc2 = nn.Linear(fc1_units, fc2_units)\n        self.V = nn.Linear(fc2_units, 1)\n        self.A = nn.Linear(fc2_units, action_size)\n\n    def forward(self, state):\n        \"\"\"Build a network that maps state -> action values.\"\"\"\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        V = self.V(x)\n        A = self.A(x)\n\n        if self.update_type == 1:\n            normalization_term = A.mean(dim=1, keepdim=True)\n        else:\n            normalization_term, _ = A.max(dim=1, keepdim=True)\n\n        Q = V + (A - normalization_term)\n        return Q","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:39:07.408163Z","iopub.execute_input":"2024-04-06T14:39:07.408566Z","iopub.status.idle":"2024-04-06T14:39:07.418622Z","shell.execute_reply.started":"2024-04-06T14:39:07.408537Z","shell.execute_reply":"2024-04-06T14:39:07.417615Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nclass ReplayBuffer:\n    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        \"\"\"Initialize a ReplayBuffer object.\n\n        Params\n        ======\n            action_size (int): dimension of each action\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n            seed (int): random seed\n        \"\"\"\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        self.seed = random.seed(seed)\n\n    def add(self, state, action, reward, next_state, done):\n        \"\"\"Add a new experience to memory.\"\"\"\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):\n        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        \"\"\"Return the current size of internal memory.\"\"\"\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:39:07.964998Z","iopub.execute_input":"2024-04-06T14:39:07.965644Z","iopub.status.idle":"2024-04-06T14:39:07.978886Z","shell.execute_reply.started":"2024-04-06T14:39:07.965609Z","shell.execute_reply":"2024-04-06T14:39:07.977850Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"class DDQNAgent():\n\n    def __init__(self, state_size, action_size, seed, update_type, action_policy, buffer_size, batch_size, lr, update_every):\n\n        ''' Agent Environment Interaction '''\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(seed)\n        self.update_type = update_type\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n        self.lr = lr\n        self.update_every = update_every\n        self.action_policy = action_policy\n\n        ''' Q-Network '''\n        self.qnetwork_local = DuellingDQN(state_size, action_size, seed, update_type = update_type).to(device)\n        self.qnetwork_target = DuellingDQN(state_size, action_size, seed, update_type = update_type).to(device)\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n\n        ''' Replay memory '''\n        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n\n        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n        self.t_step = 0\n\n    def step(self, state, action, reward, next_state, done):\n\n        ''' Save experience in replay memory '''\n        self.memory.add(state, action, reward, next_state, done)\n\n        ''' If enough samples are available in memory, get random subset and learn '''\n        if len(self.memory) >= self.batch_size:\n            experiences = self.memory.sample()\n            self.learn(experiences, GAMMA)\n\n        \"\"\" +Q TARGETS PRESENT \"\"\"\n        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n        self.t_step = (self.t_step + 1) % self.update_every\n        if self.t_step == 0:\n\n            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n\n    def act(self, state, param=0.):\n\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n\n        if self.action_policy == 'egreedy':\n            ''' Epsilon-greedy action selection (Already Present) '''\n            if random.random() > param:\n                action = np.argmax(action_values.cpu().data.numpy())\n            else:\n                action = random.choice(np.arange(self.action_size))\n\n        elif self.action_policy == 'softmax':\n            ''' softmax action selection (Task 1b Solution) '''\n            action_probs = F.softmax(action_values/param, dim = 1).cpu().data.numpy().squeeze()\n            action = np.random.choice(np.arange(self.action_size), p = action_probs)\n\n        return action\n\n    def learn(self, experiences, gamma):\n        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n        states, actions, rewards, next_states, dones = experiences\n\n        ''' Get max predicted Q values (for next states) from target model'''\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n\n        ''' Compute Q targets for current states '''\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        ''' Get expected Q values from local model '''\n        Q_expected = self.qnetwork_local(states).gather(1, actions)\n\n        ''' Compute loss '''\n        loss = F.mse_loss(Q_expected, Q_targets)\n\n        ''' Minimize the loss '''\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        ''' Gradiant Clipping '''\n        \"\"\" +T TRUNCATION PRESENT \"\"\"\n        for param in self.qnetwork_local.parameters():\n            param.grad.data.clamp_(-1, 1)\n\n        self.optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:39:09.361686Z","iopub.execute_input":"2024-04-06T14:39:09.362384Z","iopub.status.idle":"2024-04-06T14:39:09.382949Z","shell.execute_reply.started":"2024-04-06T14:39:09.362350Z","shell.execute_reply":"2024-04-06T14:39:09.382018Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"#### Defining DuellingDQN Algorithm","metadata":{"execution":{"iopub.status.busy":"2024-04-06T11:09:48.968073Z","iopub.execute_input":"2024-04-06T11:09:48.968433Z","iopub.status.idle":"2024-04-06T11:09:48.978773Z","shell.execute_reply.started":"2024-04-06T11:09:48.968406Z","shell.execute_reply":"2024-04-06T11:09:48.977612Z"}}},{"cell_type":"code","source":"\ndef ddqn(env, agent, n_episodes, max_t, p_start, p_end, p_decay, action_policy):\n\n    scores_window = deque(maxlen=100)\n    ''' last 100 scores for checking if the avg is more than 195 '''\n    scores = [] # store scores to plot the reward curve\n\n    param = p_start\n\n    for i_episode in range(1, n_episodes+1):\n        state_temp = env.reset()\n        state = state_temp[0]\n        score = 0\n        for t in range(max_t):\n            action = agent.act(state, param)\n            next_state, reward, done, _, _ = env.step(action)\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n            if done:\n                break\n\n        scores_window.append(score)\n        scores.append(score) # append scores\n\n        param = max(p_end, p_decay*param)\n\n        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n\n        if i_episode % 100 == 0:\n            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n\n    # Plot the reward curve\n    plt.plot(np.arange(len(scores)), scores)\n    plt.ylabel('Score')\n    plt.xlabel('Episode #')\n    plt.title('Reward Curve')\n    plt.show()\n\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:39:10.733881Z","iopub.execute_input":"2024-04-06T14:39:10.734767Z","iopub.status.idle":"2024-04-06T14:39:10.745249Z","shell.execute_reply.started":"2024-04-06T14:39:10.734731Z","shell.execute_reply":"2024-04-06T14:39:10.744088Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# logging in to wandb to record sweeps\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T20:15:04.914194Z","iopub.execute_input":"2024-04-06T20:15:04.914483Z","iopub.status.idle":"2024-04-06T20:15:16.365092Z","shell.execute_reply.started":"2024-04-06T20:15:04.914458Z","shell.execute_reply":"2024-04-06T20:15:16.364249Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# sweep configuration for wandb\nsweep_config = {\n    'program': 'AE20B034_EE20B092_PA2.ipynb',\n    'method': 'bayes',\n    'name': \"complete-sweep\",\n    'metric':{\n        'name': 'returns',\n        'goal': 'maximize'\n    },\n'parameters':{\n    'action_policy':\n      {'values': ['softmax']},\n    'buffer_size':\n        {'values':[int(1e5)]},\n    'batch_size':\n        {'values':[128]},\n    'lr':\n        {'values':[0.01, 0.001, 0.0001]},\n    'update_every':\n        {'values':[20]},\n    'p_start':\n        {'values':[1.2, 1, 0.9, 0.5, 0.2]},\n    'p_end':\n        {'values':[0.005]},\n    'p_decay':\n        {'values':[1, 0.995]},\n}\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wandb_exp(env_name, update_type = 2, n_episodes=5000, max_t=1000, seed = 0):\n    # initializing wandb\n    run = wandb.init()\n    config = wandb.config\n    \n    scores = run_exp(env_name, config, update_type, n_episodes, max_t, seed)\n    returns = np.mean(scores)\n    \n    wandb.log({\"returns\": returns})\n\n    return returns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_wandb(env_name, update_type, n_episodes, max_t, seed, title):\n    # run wandb \n    def exp_run():\n        wandb_exp(env_name, update_type, n_episodes, max_t, seed)\n    #     \n    sweep_id = wandb.sweep(sweep_config, project=title)\n    wandb.agent(sweep_id, function=exp_run, count = 24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_wandb('Acrobot-v1', 1, 1000, 1000, 7, 'env1_update1')\nrun_wandb('Acrobot-v1', 2, 1000, 1000, 7, 'env1_update2')\nrun_wandb('CartPole-v1', 1, 1000, 500, 7, 'env2_update1')\nrun_wandb('CartPole-v1', 2, 1000, 500, 7, 'env2_update2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperparams = {\n    'action_policy': 'softmax',\n    'buffer_size': int(1e5),\n    'batch_size': 128,\n    'lr': 0.0001,\n    'update_every': 20,\n    'p_start': 1.2,\n    'p_end': 0.005,\n    'p_decay': 0.995\n}\n\nrun_exp_plot('CartPole-v1', hyperparams, n_episodes = 500, max_t = 200)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T15:25:26.071169Z","iopub.status.idle":"2024-04-06T15:25:26.071482Z","shell.execute_reply.started":"2024-04-06T15:25:26.071326Z","shell.execute_reply":"2024-04-06T15:25:26.071340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# REINFORCE","metadata":{}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size, dropout):\n        super(Policy, self).__init__()\n        self.action_size = action_size\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc2 = nn.Linear(hidden_size, action_size)\n        self.f = nn.Linear(hidden_size, hidden_size)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def forward(self, state):\n        # Define the forward pass of the neural network\n        x = self.fc1(state)\n        x = self.dropout(x)\n        x = F.relu(x)\n        x = self.f(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return F.softmax(x, dim=-1)\n\n    def act(self, state):\n        # Perform an action based on the given state\n        with torch.no_grad():\n            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n            probs = self.forward(state).cpu()\n            model = Categorical(probs)\n            action = model.sample()\n        return action.item(), model.log_prob(action)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:51.099405Z","iopub.execute_input":"2024-04-07T06:32:51.099758Z","iopub.status.idle":"2024-04-07T06:32:51.109452Z","shell.execute_reply.started":"2024-04-07T06:32:51.099730Z","shell.execute_reply":"2024-04-07T06:32:51.108505Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def calculate_returns(rewards, discount_factor):\n    # Calculate discounted returns for the given rewards\n    discounted_returns = []\n    G = 0\n    for R in reversed(rewards):\n        G = R + discount_factor * G\n        discounted_returns = [G, *discounted_returns]\n    return discounted_returns\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:51.269968Z","iopub.execute_input":"2024-04-07T06:32:51.270241Z","iopub.status.idle":"2024-04-07T06:32:51.275249Z","shell.execute_reply.started":"2024-04-07T06:32:51.270219Z","shell.execute_reply":"2024-04-07T06:32:51.274280Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def calc_log_prob(model, state, action):\n    # Calculate the log probability of an action given a state and a model\n    probs = model.forward(state).cpu()\n    m = Categorical(probs)\n    return m.log_prob(torch.tensor(action))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:51.454262Z","iopub.execute_input":"2024-04-07T06:32:51.454543Z","iopub.status.idle":"2024-04-07T06:32:51.459347Z","shell.execute_reply.started":"2024-04-07T06:32:51.454520Z","shell.execute_reply":"2024-04-07T06:32:51.458278Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Value(nn.Module):\n    def __init__(self, state_size, hidden_size, dropout):\n        super(Value, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.dropout = nn.Dropout(p=dropout)\n        self.f = nn.Linear(hidden_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 1)\n\n    def forward(self, state):\n        # Define the forward pass of the neural network\n        x = self.fc1(state)\n        x = self.dropout(x)\n        x = F.relu(x)\n        x = self.f(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:52.028091Z","iopub.execute_input":"2024-04-07T06:32:52.028813Z","iopub.status.idle":"2024-04-07T06:32:52.035502Z","shell.execute_reply.started":"2024-04-07T06:32:52.028782Z","shell.execute_reply":"2024-04-07T06:32:52.034572Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train(policy_model, value_model, policy_optimizer, value_optimizer, rewards, states, actions, device, is_baseline):\n    # Calculate discounted returns\n    with torch.no_grad():\n        discounted_returns = torch.tensor(calculate_returns(rewards, 0.99)).to(device).view(-1, 1)\n    \n    log_probs = []\n    values = []\n    policy_loss = []\n    \n    # Calculate log probabilities for actions taken\n    for state, action in zip(states, actions):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        log_probs.append(calc_log_prob(policy_model, state, action).to(device))\n        \n    if is_baseline:\n        t_states = torch.tensor(states, dtype=torch.float32).to(device)\n        values.append(value_model.forward(t_states))\n        \n        # Calculate advantage and policy loss\n        with torch.no_grad():\n            deltas = discounted_returns - torch.cat(values)\n        for log_prob, delta in zip(log_probs, deltas):\n            policy_loss.append(-log_prob * delta)\n        policy_loss = torch.cat(policy_loss).mean()\n\n        # Update policy parameters\n        policy_optimizer.zero_grad()\n        policy_loss.backward()\n        policy_optimizer.step()\n        \n        # Calculate value loss and update value network parameters\n        value_loss = F.mse_loss(torch.cat(values), discounted_returns)\n        value_optimizer.zero_grad()\n        value_loss.backward()\n        value_optimizer.step()\n    else:\n        # Calculate policy loss without baseline\n        for log_prob, ret in zip(log_probs, discounted_returns):\n            policy_loss.append(-log_prob * ret)\n        policy_loss = torch.cat(policy_loss).mean()\n        policy_optimizer.zero_grad()\n        policy_loss.backward()\n        policy_optimizer.step()\n    \n    return\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:53.589846Z","iopub.execute_input":"2024-04-07T06:32:53.590201Z","iopub.status.idle":"2024-04-07T06:32:53.601543Z","shell.execute_reply.started":"2024-04-07T06:32:53.590173Z","shell.execute_reply":"2024-04-07T06:32:53.600586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def run(env, n_episodes, update_type, max_t, policy_lr, value_lr, hidden_size, dropout):\n    # Initialize an empty list to store rewards\n    rewards = []\n    # Determine the device for computation\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Initialize policy and value networks\n    policy_model = Policy(env.observation_space.shape[0], env.action_space.n, hidden_size, dropout).to(device)\n    value_model = Value(env.observation_space.shape[0], hidden_size, dropout).to(device)\n    # Initialize optimizers\n    policy_optimizer = optim.Adam(policy_model.parameters(), lr=policy_lr)\n    value_optimizer = optim.Adam(value_model.parameters(), lr=value_lr)\n    # Check if baseline should be used\n    is_baseline = bool(update_type - 1)\n\n    # Run episodes\n    for episode in tqdm(range(n_episodes)):\n        state = env.reset()[0]\n        rewards_episode = []  # Store rewards for each episode\n        policy_loss = []  # Store policy loss for each step\n        action_episode = []  # Store actions taken for each step\n        state_episode = []  # Store states observed for each step\n        for t in range(max_t):\n            action, log_probs = policy_model.act(state)\n            next_state, reward, done, _, _ = env.step(action)\n            rewards_episode.append(reward)\n            state_episode.append(state)\n            action_episode.append(action)\n            state = next_state\n            if done:\n                break\n        rewards.append(sum(rewards_episode))  # Store total reward for the episode\n        # Train the policy and value networks using collected data\n        train(policy_model, value_model, policy_optimizer, value_optimizer, rewards_episode, state_episode, action_episode, device, is_baseline)\n    \n    return rewards\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:32:54.211947Z","iopub.execute_input":"2024-04-07T06:32:54.212642Z","iopub.status.idle":"2024-04-07T06:32:54.222079Z","shell.execute_reply.started":"2024-04-07T06:32:54.212604Z","shell.execute_reply":"2024-04-07T06:32:54.221144Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def run_exp(env_name, algo, hyperparams, update_type=2, n_episodes=500, max_t=200, seed=0):\n    # Create the environment\n    env = gym.make(env_name)\n    state_shape = env.observation_space.shape[0]\n    action_shape = env.action_space.n\n    \n    if algo == 'ddqn':\n        # Extract hyperparameters for DDQN\n        action_policy = hyperparams['action_policy']\n        buffer_size = hyperparams['buffer_size']\n        batch_size = hyperparams['batch_size']\n        lr = hyperparams['lr']\n        update_every = hyperparams['update_every']\n        p_start = hyperparams['p_start']\n        p_end = hyperparams['p_end']\n        p_decay = hyperparams['p_decay']\n        \n        # Initialize DDQN agent\n        agent = DDQNAgent(state_size=state_shape, action_size=action_shape, seed=seed, update_type=update_type, action_policy=action_policy, buffer_size=buffer_size, batch_size=batch_size, lr=lr, update_every=update_every)\n        \n        # Run DDQN training\n        scores = ddqn(env, agent, n_episodes=n_episodes, max_t=max_t, p_start=p_start, p_end=p_end, p_decay=p_decay, action_policy=action_policy)\n    \n    elif algo == 'reinforce':\n        # Extract hyperparameters for REINFORCE\n        policy_lr = hyperparams['policy_lr']\n        value_lr = hyperparams['value_lr']\n        hidden_size = hyperparams['hidden_size']\n        dropout = hyperparams['dropout']\n        \n        # Run REINFORCE training\n        scores = run(env, n_episodes, update_type, max_t, policy_lr, value_lr, hidden_size, dropout)\n    \n    else:\n        scores = 0\n        print(\"error\")\n    \n    return scores\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:34:12.659317Z","iopub.execute_input":"2024-04-07T06:34:12.660311Z","iopub.status.idle":"2024-04-07T06:34:12.669832Z","shell.execute_reply.started":"2024-04-07T06:34:12.660274Z","shell.execute_reply":"2024-04-07T06:34:12.668544Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def run_exp_plot(env_name, hyperparams, n_episodes=10000, max_t=200, algo='ddqn'):\n    # Collect scores for Update Type 1\n    scores1 = [run_exp(env_name=env_name, algo=algo, hyperparams=hyperparams, update_type=1, n_episodes=n_episodes, max_t=max_t, seed=seed)[::10] for seed in range(5)]\n    \n    # Collect scores for Update Type 2\n    scores2 = [run_exp(env_name=env_name, algo=algo, hyperparams=hyperparams, update_type=2, n_episodes=n_episodes, max_t=max_t, seed=seed)[::10] for seed in range(5)]\n    \n    # Convert to NumPy arrays\n    scores1 = np.array(scores1)\n    scores2 = np.array(scores2)\n    \n    # Compute mean scores over seeds\n    mean_scores1 = np.mean(scores1, axis=0)\n    mean_scores2 = np.mean(scores2, axis=0)\n    \n    # Compute variance over seeds\n    std_scores1 = np.std(scores1, axis=0)\n    std_scores2 = np.std(scores2, axis=0)\n    \n    # Plot the mean scores with variance bands\n    episodes = range(1, n_episodes + 1)\n    plt.subplots(figsize=(10, 6))\n    if algo == 'ddqn':\n        # Update Type 1\n        plt.plot(mean_scores1, label='Update Type 1')\n        plt.fill_between(range(len(mean_scores1)), mean_scores1 - std_scores1, mean_scores1 + std_scores1, alpha=0.2)\n\n        # Update Type 2\n        plt.plot(mean_scores2, label='Update Type 2')\n        plt.fill_between(range(len(mean_scores2)), mean_scores2 - std_scores2, mean_scores2 + std_scores2,alpha=0.2)\n    else:\n        # Update Type 1\n        plt.plot(mean_scores1, label='w/o baseline')\n        plt.fill_between(range(len(mean_scores1)), mean_scores1 - std_scores1, mean_scores1 + std_scores1,alpha=0.2)\n\n        # Update Type 2\n        plt.plot(mean_scores2, label='w/ baseline')\n        plt.fill_between(range(len(mean_scores2)), mean_scores2 - std_scores2, mean_scores2 + std_scores2,alpha=0.2)\n\n    # Add axis labels and a legend\n    plt.xlabel('Episodes')\n    plt.ylabel('Episodic Returns')\n    plt.title(f'Returns vs. Episodes for {env_name} Environment - {algo}')\n    plt.legend()\n\n    # Save the plot\n    plt.savefig(f'{env_name}_{algo}_scores.png', dpi=300, bbox_inches='tight')\n\n    # Show the plot\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:34:13.033721Z","iopub.execute_input":"2024-04-07T06:34:13.034318Z","iopub.status.idle":"2024-04-07T06:34:13.047195Z","shell.execute_reply.started":"2024-04-07T06:34:13.034292Z","shell.execute_reply":"2024-04-07T06:34:13.046148Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Hyper parameters","metadata":{}},{"cell_type":"code","source":"# logging in to wandb to record sweeps\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:40:50.999247Z","iopub.execute_input":"2024-04-06T21:40:50.999515Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"}]},{"cell_type":"code","source":"# sweep configuration for wandb\nsweep_config = {\n    'program': 'AE20B034_EE20B092_PA2.ipynb',\n    'method': 'bayes',\n    'name': \"complete-sweep\",\n    'metric':{\n        'name': 'returns',\n        'goal': 'maximize'\n    },\n'parameters':{\n    'policy_lr':\n      {'values': [0.005, 0.001]},\n    'value_lr':\n      {'values': [0.005, 0.001]},\n    'hidden_size':\n        {'values':[64, 128, 256]},\n    'dropout':\n        {'values':[0.0, 0.25, 0.5]},\n}\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wandb_exp(env_name, update_type = 2, n_episodes=5000, max_t=1000, seed = 0, algo = 'ddqn'):\n    # initializing wandb\n    run = wandb.init()\n    config = wandb.config\n    scores = run_exp(env_name, algo, config, update_type, n_episodes = n_episodes, max_t = max_t, seed = seed)\n#     scores = run_exp(env_name, config, algo, update_type, n_episodes, max_t, seed)\n    returns = np.mean(scores)\n    \n    wandb.log({\"returns\": returns})\n\n    return returns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_wandb(env_name, update_type, n_episodes, max_t, seed, title, algo):\n\n    def exp_run():\n        \n        wandb_exp(env_name, update_type, n_episodes, max_t, seed, algo)\n        \n    sweep_id = wandb.sweep(sweep_config, project=title)\n    wandb.agent(sweep_id, function=exp_run, count = 24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_wandb('Acrobot-v1', 1, 500, 200, 7, 'env1_update1', 'reinforce')\nrun_wandb('Acrobot-v1', 2, 500, 200, 7, 'env1_update2', 'reinforce')\nrun_wandb('CartPole-v1', 1, 500, 200, 7, 'env2_update1', 'reinforce')\nrun_wandb('CartPole-v1', 2, 500, 200, 7, 'env2_update2', 'reinforce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run environments\n","metadata":{}},{"cell_type":"code","source":"hyperparams = {\n    'action_policy': 'softmax',\n    'buffer_size': int(1e5),\n    'batch_size': 128,\n    'lr': 0.0001,\n    'update_every': 20,\n    'p_start': 0.2,\n    'p_end': 0.005,\n    'p_decay': 0.995\n}\n\nrun_exp_plot('Acrobot-v1', hyperparams, n_episodes = 1000, max_t = 200, algo = 'ddqn')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T12:34:29.557104Z","iopub.execute_input":"2024-04-06T12:34:29.557528Z","iopub.status.idle":"2024-04-06T12:34:29.563866Z","shell.execute_reply.started":"2024-04-06T12:34:29.557480Z","shell.execute_reply":"2024-04-06T12:34:29.562970Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"hyperparams = {\n    'action_policy': 'softmax',\n    'buffer_size': int(1e5),\n    'batch_size': 128,\n    'lr': 0.0001,\n    'update_every': 20,\n    'p_start': 1.2,\n    'p_end': 0.005,\n    'p_decay': 0.995\n}\n\nrun_exp_plot('CartPole-v1', hyperparams, n_episodes = 500, max_t = 500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperparams = {\n    'policy_lr': 1e-3,\n    'value_lr': 1e-3,\n    'hidden_size': 256,\n    'dropout': 0.0\n}\n\nrun_exp_plot('Acrobot-v1', hyperparams, n_episodes = 500, max_t = 500, algo = 'reinforce')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperparams = {\n    'policy_lr': 1e-3,\n    'value_lr': 1e-3,\n    'hidden_size': 128,\n    'dropout': 0.25\n}\n\nrun_exp_plot('Cartpole-v1', hyperparams, n_episodes = 500, max_t = 500, algo = 'reinforce')","metadata":{},"execution_count":null,"outputs":[]}]}